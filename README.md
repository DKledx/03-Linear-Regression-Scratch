# ğŸ“ˆ Build Your Own Linear Regression

## ğŸ¯ Má»¥c TiÃªu Dá»± Ãn

Dá»± Ã¡n **Linear Regression from Scratch** sáº½ dáº¡y báº¡n hiá»ƒu sÃ¢u vá» thuáº­t toÃ¡n há»“i quy tuyáº¿n tÃ­nh báº±ng cÃ¡ch tá»± implement tá»« Ä‘áº§u. Báº¡n sáº½ code gradient descent, cost function, vÃ  toÃ n bá»™ quÃ¡ trÃ¬nh training mÃ  khÃ´ng dÃ¹ng thÆ° viá»‡n cÃ³ sáºµn. ÄÃ¢y lÃ  ná»n táº£ng quan trá»ng Ä‘á»ƒ hiá»ƒu machine learning algorithms.

## ğŸ“ Kiáº¿n Thá»©c Sáº½ Há»c ÄÆ°á»£c

### ğŸ“š Mathematical Foundations
- **Linear Algebra**: Vector operations, matrix multiplication
- **Calculus**: Derivatives, partial derivatives
- **Statistics**: Mean, variance, correlation
- **Optimization**: Gradient descent algorithm

### ğŸ”§ Core Concepts
- **Linear Regression**: Há»“i quy tuyáº¿n tÃ­nh
- **Cost Function**: HÃ m máº¥t mÃ¡t (MSE, MAE)
- **Gradient Descent**: Thuáº­t toÃ¡n tá»‘i Æ°u
- **Learning Rate**: Tá»‘c Ä‘á»™ há»c
- **Feature Scaling**: Chuáº©n hÃ³a dá»¯ liá»‡u

### ğŸ› ï¸ Implementation Skills
- **NumPy**: Numerical computing
- **Vectorization**: Tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n
- **Object-Oriented Programming**: Class design
- **Debugging**: TÃ¬m vÃ  sá»­a lá»—i

## ğŸ“ Cáº¥u TrÃºc Dá»± Ãn

```
03-Linear-Regression-Scratch/
â”œâ”€â”€ README.md
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01-mathematical-foundations.ipynb
â”‚   â”œâ”€â”€ 02-simple-linear-regression.ipynb
â”‚   â”œâ”€â”€ 03-multiple-linear-regression.ipynb
â”‚   â”œâ”€â”€ 04-gradient-descent-deep-dive.ipynb
â”‚   â”œâ”€â”€ 05-regularization.ipynb
â”‚   â””â”€â”€ 06-comparison-with-sklearn.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”œâ”€â”€ gradient_descent.py
â”‚   â”œâ”€â”€ cost_functions.py
â”‚   â”œâ”€â”€ data_generator.py
â”‚   â””â”€â”€ visualization.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ synthetic/
â”‚   â””â”€â”€ real_world/
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_linear_regression.py
â”‚   â””â”€â”€ test_gradient_descent.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore
```

## ğŸ“Š Dataset Overview

### ğŸ² Synthetic Data
- **Linear Data**: Y = 2X + 3 + noise
- **Multiple Features**: Y = 3X1 + 2X2 - X3 + 5 + noise
- **Non-linear Data**: Y = XÂ² + 2X + 1 + noise (Ä‘á»ƒ test limitations)

### ğŸ  Real-world Data
- **Housing Dataset**: sklearn.datasets.fetch_california_housing
- **Boston Housing**: sklearn.datasets.load_boston (deprecated)
- **Custom Dataset**: Táº¡o tá»« Kaggle

## ğŸš€ CÃ¡ch Báº¯t Äáº§u

### 1. CÃ i Äáº·t MÃ´i TrÆ°á»ng
```bash
# Táº¡o virtual environment
python -m venv lr_env
source lr_env/bin/activate  # Linux/Mac
# hoáº·c
lr_env\Scripts\activate     # Windows

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### 2. Cháº¡y Jupyter Notebook
```bash
jupyter notebook
```

### 3. Báº¯t Äáº§u vá»›i Mathematical Foundations
Má»Ÿ `notebooks/01-mathematical-foundations.ipynb` Ä‘á»ƒ hiá»ƒu toÃ¡n há»c Ä‘áº±ng sau!

### 4. Cháº¡y test Ä‘á»ƒ xÃ¡c nháº­n implementation
```bash
pytest -q
```

## ğŸ“‹ Roadmap Há»c Táº­p

### âœ… Phase 1: Mathematical Foundations
- [ ] Linear algebra basics
- [ ] Calculus for machine learning
- [ ] Cost functions (MSE, MAE)
- [ ] Gradient descent intuition

> LÆ°u Ã½: Báº¡n cÃ³ thá»ƒ tick cÃ¡c má»¥c nÃ y sau khi hoÃ n thÃ nh tá»«ng notebook tÆ°Æ¡ng á»©ng.

### âœ… Phase 2: Simple Linear Regression
- [ ] Implement from scratch
- [ ] Visualize cost function
- [ ] Gradient descent implementation
- [ ] Convergence analysis

### âœ… Phase 3: Multiple Linear Regression
- [ ] Vectorized implementation
- [ ] Feature scaling
- [ ] Normal equation
- [ ] Performance comparison

### âœ… Phase 4: Advanced Topics
- [ ] Regularization (Ridge, Lasso)
- [ ] Learning rate scheduling
- [ ] Batch vs Stochastic gradient descent
- [ ] Feature engineering

### âœ… Phase 5: Validation & Testing
- [ ] Unit tests
- [ ] Comparison with sklearn
- [ ] Real-world applications
- [ ] Performance optimization

## ğŸ§® Mathematical Implementation

### ğŸ“ Simple Linear Regression
```python
class SimpleLinearRegression:
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X, y):
        # Initialize parameters
        self.weights = np.random.normal(0, 0.01, X.shape[1])
        self.bias = 0
        
        # Gradient descent
        for i in range(self.max_iterations):
            # Forward pass
            y_pred = self.predict(X)
            
            # Compute cost
            cost = self.compute_cost(y, y_pred)
            self.cost_history.append(cost)
            
            # Compute gradients
            dw = (1/len(X)) * np.dot(X.T, (y_pred - y))
            db = (1/len(X)) * np.sum(y_pred - y)
            
            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        return np.dot(X, self.weights) + self.bias
    
    def compute_cost(self, y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)
```

> Gá»£i Ã½ thá»±c hÃ nh: thá»­ thay Ä‘á»•i `learning_rate`, `max_iterations` vÃ  quan sÃ¡t Ä‘Æ°á»ng cong há»c `cost_history`.

### ğŸ“Š Cost Function Visualization
```python
def plot_cost_function(X, y, weight_range, bias_range):
    """Visualize cost function in 3D"""
    W, B = np.meshgrid(weight_range, bias_range)
    costs = np.zeros_like(W)
    
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            y_pred = W[i,j] * X + B[i,j]
            costs[i,j] = np.mean((y - y_pred) ** 2)
    
    fig = plt.figure(figsize=(12, 5))
    
    # 3D surface plot
    ax1 = fig.add_subplot(121, projection='3d')
    ax1.plot_surface(W, B, costs, alpha=0.7, cmap='viridis')
    ax1.set_xlabel('Weight')
    ax1.set_ylabel('Bias')
    ax1.set_zlabel('Cost')
    ax1.set_title('Cost Function Surface')
    
    # Contour plot
    ax2 = fig.add_subplot(122)
    contour = ax2.contour(W, B, costs, levels=20)
    ax2.clabel(contour, inline=True, fontsize=8)
    ax2.set_xlabel('Weight')
    ax2.set_ylabel('Bias')
    ax2.set_title('Cost Function Contours')
    
    plt.tight_layout()
    plt.show()
```

## ğŸ¯ Gradient Descent Variants

### ğŸ“ˆ Batch Gradient Descent
```python
def batch_gradient_descent(X, y, learning_rate=0.01, max_iterations=1000):
    """Standard gradient descent using all data"""
    m = len(y)
    theta = np.random.normal(0, 0.01, X.shape[1])
    cost_history = []
    
    for i in range(max_iterations):
        # Compute predictions
        y_pred = np.dot(X, theta)
        
        # Compute cost
        cost = np.mean((y - y_pred) ** 2)
        cost_history.append(cost)
        
        # Compute gradient
        gradient = (1/m) * np.dot(X.T, (y_pred - y))
        
        # Update parameters
        theta -= learning_rate * gradient
    
    return theta, cost_history
```

### âš¡ Stochastic Gradient Descent
```python
def stochastic_gradient_descent(X, y, learning_rate=0.01, max_epochs=100):
    """SGD using one sample at a time"""
    m = len(y)
    theta = np.random.normal(0, 0.01, X.shape[1])
    cost_history = []
    
    for epoch in range(max_epochs):
        # Shuffle data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        epoch_cost = 0
        for i in range(m):
            # Single sample
            x_i = X_shuffled[i:i+1]
            y_i = y_shuffled[i:i+1]
            
            # Compute prediction
            y_pred = np.dot(x_i, theta)
            
            # Compute cost
            cost = (y_i - y_pred) ** 2
            epoch_cost += cost[0]
            
            # Compute gradient
            gradient = np.dot(x_i.T, (y_pred - y_i))
            
            # Update parameters
            theta -= learning_rate * gradient
        
        cost_history.append(epoch_cost / m)
    
    return theta, cost_history
```

## ğŸ“Š Evaluation & Visualization

### ğŸ¯ Model Performance
```python
def evaluate_model(y_true, y_pred):
    """Comprehensive model evaluation"""
    mse = np.mean((y_true - y_pred) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(y_true - y_pred))
    r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))
    
    print(f"Mean Squared Error: {mse:.4f}")
    print(f"Root Mean Squared Error: {rmse:.4f}")
    print(f"Mean Absolute Error: {mae:.4f}")
    print(f"RÂ² Score: {r2:.4f}")
    
    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}
```

### ğŸ” Máº¹o Ä‘Ã¡nh giÃ¡ nhanh
- **MSE/RMSE tháº¥p**: mÃ´ hÃ¬nh khá»›p tá»‘t dá»¯ liá»‡u.
- **RÂ² gáº§n 1**: mÃ´ hÃ¬nh giáº£i thÃ­ch tá»‘t phÆ°Æ¡ng sai cá»§a dá»¯ liá»‡u.

### ğŸ“ˆ Learning Curves
```python
def plot_learning_curves(cost_history, title="Learning Curve"):
    """Plot cost function over iterations"""
    plt.figure(figsize=(10, 6))
    plt.plot(cost_history)
    plt.title(title)
    plt.xlabel('Iterations')
    plt.ylabel('Cost')
    plt.grid(True)
    plt.show()
```

## ğŸ”§ Advanced Features

### ğŸ›ï¸ Regularization
```python
class RidgeRegression:
    def __init__(self, alpha=1.0, learning_rate=0.01):
        self.alpha = alpha  # Regularization parameter
        self.learning_rate = learning_rate
        self.weights = None
        self.bias = None
    
    def fit(self, X, y):
        # Ridge regression with L2 regularization
        m = len(y)
        self.weights = np.random.normal(0, 0.01, X.shape[1])
        self.bias = 0
        
        for i in range(1000):
            y_pred = self.predict(X)
            
            # Cost with regularization
            cost = np.mean((y - y_pred) ** 2) + self.alpha * np.sum(self.weights ** 2)
            
            # Gradients with regularization
            dw = (1/m) * np.dot(X.T, (y_pred - y)) + 2 * self.alpha * self.weights
            db = (1/m) * np.sum(y_pred - y)
            
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
```

> Thá»±c hÃ nh: thay Ä‘á»•i `alpha` Ä‘á»ƒ tháº¥y tÃ¡c Ä‘á»™ng cá»§a regularization lÃªn trá»ng sá»‘.

## ğŸ“ Notes Thá»±c Thi Dá»± Ãn

- **Quy Æ°á»›c mÃ´i trÆ°á»ng**: dÃ¹ng `lr_env` cho riÃªng dá»± Ã¡n nÃ y.
- **Cháº¡y test trÆ°á»›c commit**: Ä‘áº£m báº£o `pytest` pass 100%.
- **Kiá»ƒm thá»­ thá»§ cÃ´ng**: cháº¡y notebook `02-simple-linear-regression` Ä‘á»ƒ xem Ä‘Æ°á»ng há»“i quy vÃ  learning curve.
- **Quy táº¯c dá»¯ liá»‡u**: dá»¯ liá»‡u synthetic náº±m á»Ÿ `data/synthetic/`, dá»¯ liá»‡u tháº­t á»Ÿ `data/real_world/`.
- **Quáº£n lÃ½ phá»¥ thuá»™c**: cáº­p nháº­t `requirements.txt` khi thÃªm thÆ° viá»‡n má»›i.
- **Äá»‹nh dáº¡ng code**: cÃ³ thá»ƒ dÃ¹ng `black` vÃ  `flake8` (Ä‘Ã£ khai bÃ¡o trong requirements).

## ğŸ§ª HÆ°á»›ng Dáº«n Test Nhanh

```bash
python - << 'PY'
import numpy as np
from src.linear_regression import SimpleLinearRegression

X = np.linspace(-5,5,200).reshape(-1,1)
y = 2*X.squeeze() + 3 + np.random.normal(0,0.2,200)

model = SimpleLinearRegression(learning_rate=0.05, max_iterations=5000, tolerance=1e-10, random_state=42)
model.fit(X,y)
print(model.get_parameters())
print('R2=', model.score(X,y))
PY
```

## ğŸ¯ Expected Results

Sau khi hoÃ n thÃ nh dá»± Ã¡n nÃ y, báº¡n sáº½ cÃ³:

1. **Deep Understanding**: Hiá»ƒu sÃ¢u vá» linear regression
2. **Implementation Skills**: Tá»± code Ä‘Æ°á»£c thuáº­t toÃ¡n
3. **Mathematical Intuition**: Hiá»ƒu toÃ¡n há»c Ä‘áº±ng sau
4. **Optimization Knowledge**: Gradient descent variants
5. **Performance Comparison**: So sÃ¡nh vá»›i sklearn

## ğŸ” Key Insights to Discover

### ğŸ“Š Mathematical Insights
- **Cost function shape** áº£nh hÆ°á»Ÿng Ä‘áº¿n convergence
- **Learning rate** quyáº¿t Ä‘á»‹nh tá»‘c Ä‘á»™ há»c
- **Feature scaling** quan trá»ng cho gradient descent
- **Regularization** giÃºp trÃ¡nh overfitting

### ğŸ¯ Implementation Insights
- **Vectorization** tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ
- **Batch size** áº£nh hÆ°á»Ÿng Ä‘áº¿n stability
- **Initialization** quan trá»ng cho convergence
- **Early stopping** trÃ¡nh overfitting

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- [Andrew Ng's Machine Learning Course](https://www.coursera.org/learn/machine-learning)
- [Linear Regression Mathematics](https://en.wikipedia.org/wiki/Linear_regression)
- [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)
- [NumPy Documentation](https://numpy.org/doc/)

## ğŸ† Next Steps

Sau khi hoÃ n thÃ nh Linear Regression from Scratch, báº¡n cÃ³ thá»ƒ:
- Implement other algorithms from scratch
- Chuyá»ƒn sang dá»± Ã¡n 4: Titanic Survival Prediction
- Thá»­ advanced optimization techniques
- PhÃ¡t triá»ƒn mini ML library

## ğŸ¨ Bonus: Interactive Demo

Táº¡o Streamlit app Ä‘á»ƒ demo:
- Adjust learning rate vÃ  iterations
- Visualize gradient descent in real-time
- Compare different algorithms
- Upload custom datasets

---

**Happy Coding! ğŸ“ˆ**

*HÃ£y báº¯t Ä‘áº§u vá»›i mathematical foundations vÃ  xÃ¢y dá»±ng linear regression tá»« con sá»‘ 0!*
