# üìà Build Your Own Linear Regression

## üéØ M·ª•c Ti√™u D·ª± √Ån

D·ª± √°n **Linear Regression from Scratch** s·∫Ω d·∫°y b·∫°n hi·ªÉu s√¢u v·ªÅ thu·∫≠t to√°n h·ªìi quy tuy·∫øn t√≠nh b·∫±ng c√°ch t·ª± implement t·ª´ ƒë·∫ßu. B·∫°n s·∫Ω code gradient descent, cost function, v√† to√†n b·ªô qu√° tr√¨nh training m√† kh√¥ng d√πng th∆∞ vi·ªán c√≥ s·∫µn. ƒê√¢y l√† n·ªÅn t·∫£ng quan tr·ªçng ƒë·ªÉ hi·ªÉu machine learning algorithms.

## üéì Ki·∫øn Th·ª©c S·∫Ω H·ªçc ƒê∆∞·ª£c

### üìö Mathematical Foundations
- **Linear Algebra**: Vector operations, matrix multiplication
- **Calculus**: Derivatives, partial derivatives
- **Statistics**: Mean, variance, correlation
- **Optimization**: Gradient descent algorithm

### üîß Core Concepts
- **Linear Regression**: H·ªìi quy tuy·∫øn t√≠nh
- **Cost Function**: H√†m m·∫•t m√°t (MSE, MAE)
- **Gradient Descent**: Thu·∫≠t to√°n t·ªëi ∆∞u
- **Learning Rate**: T·ªëc ƒë·ªô h·ªçc
- **Feature Scaling**: Chu·∫©n h√≥a d·ªØ li·ªáu

### üõ†Ô∏è Implementation Skills
- **NumPy**: Numerical computing
- **Vectorization**: T·ªëi ∆∞u h√≥a t√≠nh to√°n
- **Object-Oriented Programming**: Class design
- **Debugging**: T√¨m v√† s·ª≠a l·ªói

## üìÅ C·∫•u Tr√∫c D·ª± √Ån

```
03-Linear-Regression-Scratch/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ 01-mathematical-foundations.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02-simple-linear-regression.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03-multiple-linear-regression.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04-gradient-descent-deep-dive.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 05-regularization.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 06-comparison-with-sklearn.ipynb
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ linear_regression.py
‚îÇ   ‚îú‚îÄ‚îÄ gradient_descent.py
‚îÇ   ‚îú‚îÄ‚îÄ cost_functions.py
‚îÇ   ‚îú‚îÄ‚îÄ data_generator.py
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ synthetic/
‚îÇ   ‚îî‚îÄ‚îÄ real_world/
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_linear_regression.py
‚îÇ   ‚îî‚îÄ‚îÄ test_gradient_descent.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .gitignore
```

## üìä Dataset Overview

### üé≤ Synthetic Data
- **Linear Data**: Y = 2X + 3 + noise
- **Multiple Features**: Y = 3X1 + 2X2 - X3 + 5 + noise
- **Non-linear Data**: Y = X¬≤ + 2X + 1 + noise (ƒë·ªÉ test limitations)

### üè† Real-world Data
- **Housing Dataset**: sklearn.datasets.fetch_california_housing
- **Boston Housing**: sklearn.datasets.load_boston (deprecated)
- **Custom Dataset**: T·∫°o t·ª´ Kaggle

## üöÄ C√°ch B·∫Øt ƒê·∫ßu

### 1. C√†i ƒê·∫∑t M√¥i Tr∆∞·ªùng
```bash
# T·∫°o virtual environment
python -m venv lr_env
source lr_env/bin/activate  # Linux/Mac
# ho·∫∑c
lr_env\Scripts\activate     # Windows

# C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt
```

### 2. Ch·∫°y Jupyter Notebook
```bash
jupyter notebook
```

### 3. B·∫Øt ƒê·∫ßu v·ªõi Mathematical Foundations
M·ªü `notebooks/01-mathematical-foundations.ipynb` ƒë·ªÉ hi·ªÉu to√°n h·ªçc ƒë·∫±ng sau!

### 4. Ch·∫°y test ƒë·ªÉ x√°c nh·∫≠n implementation
```bash
pytest -q
```

## üìã Roadmap H·ªçc T·∫≠p

### ‚úÖ Phase 1: Mathematical Foundations
- [ ] Linear algebra basics
- [ ] Calculus for machine learning
- [ ] Cost functions (MSE, MAE)
- [ ] Gradient descent intuition

> L∆∞u √Ω: B·∫°n c√≥ th·ªÉ tick c√°c m·ª•c n√†y sau khi ho√†n th√†nh t·ª´ng notebook t∆∞∆°ng ·ª©ng.

### ‚úÖ Phase 2: Simple Linear Regression
- [ ] Implement from scratch
- [ ] Visualize cost function
- [ ] Gradient descent implementation
- [ ] Convergence analysis

### ‚úÖ Phase 3: Multiple Linear Regression
- [ ] Vectorized implementation
- [ ] Feature scaling
- [ ] Normal equation
- [ ] Performance comparison

### ‚úÖ Phase 4: Advanced Topics
- [ ] Regularization (Ridge, Lasso)
- [ ] Learning rate scheduling
- [ ] Batch vs Stochastic gradient descent
- [ ] Feature engineering

### ‚úÖ Phase 5: Validation & Testing
- [ ] Unit tests
- [ ] Comparison with sklearn
- [ ] Real-world applications
- [ ] Performance optimization

## üßÆ Mathematical Implementation

### üìê Simple Linear Regression
```python
class SimpleLinearRegression:
    def __init__(self, learning_rate=0.01, max_iterations=1000):
        self.learning_rate = learning_rate
        self.max_iterations = max_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
    
    def fit(self, X, y):
        # Initialize parameters
        self.weights = np.random.normal(0, 0.01, X.shape[1])
        self.bias = 0
        
        # Gradient descent
        for i in range(self.max_iterations):
            # Forward pass
            y_pred = self.predict(X)
            
            # Compute cost
            cost = self.compute_cost(y, y_pred)
            self.cost_history.append(cost)
            
            # Compute gradients
            dw = (1/len(X)) * np.dot(X.T, (y_pred - y))
            db = (1/len(X)) * np.sum(y_pred - y)
            
            # Update parameters
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
    
    def predict(self, X):
        return np.dot(X, self.weights) + self.bias
    
    def compute_cost(self, y_true, y_pred):
        return np.mean((y_true - y_pred) ** 2)
```

> G·ª£i √Ω th·ª±c h√†nh: th·ª≠ thay ƒë·ªïi `learning_rate`, `max_iterations` v√† quan s√°t ƒë∆∞·ªùng cong h·ªçc `cost_history`.

### üìä Cost Function Visualization
```python
def plot_cost_function(X, y, weight_range, bias_range):
    """Visualize cost function in 3D"""
    W, B = np.meshgrid(weight_range, bias_range)
    costs = np.zeros_like(W)
    
    for i in range(W.shape[0]):
        for j in range(W.shape[1]):
            y_pred = W[i,j] * X + B[i,j]
            costs[i,j] = np.mean((y - y_pred) ** 2)
    
    fig = plt.figure(figsize=(12, 5))
    
    # 3D surface plot
    ax1 = fig.add_subplot(121, projection='3d')
    ax1.plot_surface(W, B, costs, alpha=0.7, cmap='viridis')
    ax1.set_xlabel('Weight')
    ax1.set_ylabel('Bias')
    ax1.set_zlabel('Cost')
    ax1.set_title('Cost Function Surface')
    
    # Contour plot
    ax2 = fig.add_subplot(122)
    contour = ax2.contour(W, B, costs, levels=20)
    ax2.clabel(contour, inline=True, fontsize=8)
    ax2.set_xlabel('Weight')
    ax2.set_ylabel('Bias')
    ax2.set_title('Cost Function Contours')
    
    plt.tight_layout()
    plt.show()
```

## üéØ Gradient Descent Variants

### üìà Batch Gradient Descent
```python
def batch_gradient_descent(X, y, learning_rate=0.01, max_iterations=1000):
    """Standard gradient descent using all data"""
    m = len(y)
    theta = np.random.normal(0, 0.01, X.shape[1])
    cost_history = []
    
    for i in range(max_iterations):
        # Compute predictions
        y_pred = np.dot(X, theta)
        
        # Compute cost
        cost = np.mean((y - y_pred) ** 2)
        cost_history.append(cost)
        
        # Compute gradient
        gradient = (1/m) * np.dot(X.T, (y_pred - y))
        
        # Update parameters
        theta -= learning_rate * gradient
    
    return theta, cost_history
```

### ‚ö° Stochastic Gradient Descent
```python
def stochastic_gradient_descent(X, y, learning_rate=0.01, max_epochs=100):
    """SGD using one sample at a time"""
    m = len(y)
    theta = np.random.normal(0, 0.01, X.shape[1])
    cost_history = []
    
    for epoch in range(max_epochs):
        # Shuffle data
        indices = np.random.permutation(m)
        X_shuffled = X[indices]
        y_shuffled = y[indices]
        
        epoch_cost = 0
        for i in range(m):
            # Single sample
            x_i = X_shuffled[i:i+1]
            y_i = y_shuffled[i:i+1]
            
            # Compute prediction
            y_pred = np.dot(x_i, theta)
            
            # Compute cost
            cost = (y_i - y_pred) ** 2
            epoch_cost += cost[0]
            
            # Compute gradient
            gradient = np.dot(x_i.T, (y_pred - y_i))
            
            # Update parameters
            theta -= learning_rate * gradient
        
        cost_history.append(epoch_cost / m)
    
    return theta, cost_history
```

## üìä Evaluation & Visualization

### üéØ Model Performance
```python
def evaluate_model(y_true, y_pred):
    """Comprehensive model evaluation"""
    mse = np.mean((y_true - y_pred) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(y_true - y_pred))
    r2 = 1 - (np.sum((y_true - y_pred) ** 2) / np.sum((y_true - np.mean(y_true)) ** 2))
    
    print(f"Mean Squared Error: {mse:.4f}")
    print(f"Root Mean Squared Error: {rmse:.4f}")
    print(f"Mean Absolute Error: {mae:.4f}")
    print(f"R¬≤ Score: {r2:.4f}")
    
    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}
```

### üîé M·∫πo ƒë√°nh gi√° nhanh
- **MSE/RMSE th·∫•p**: m√¥ h√¨nh kh·ªõp t·ªët d·ªØ li·ªáu.
- **R¬≤ g·∫ßn 1**: m√¥ h√¨nh gi·∫£i th√≠ch t·ªët ph∆∞∆°ng sai c·ªßa d·ªØ li·ªáu.

### üìà Learning Curves
```python
def plot_learning_curves(cost_history, title="Learning Curve"):
    """Plot cost function over iterations"""
    plt.figure(figsize=(10, 6))
    plt.plot(cost_history)
    plt.title(title)
    plt.xlabel('Iterations')
    plt.ylabel('Cost')
    plt.grid(True)
    plt.show()
```

## üîß Advanced Features

### üéõÔ∏è Regularization
```python
class RidgeRegression:
    def __init__(self, alpha=1.0, learning_rate=0.01):
        self.alpha = alpha  # Regularization parameter
        self.learning_rate = learning_rate
        self.weights = None
        self.bias = None
    
    def fit(self, X, y):
        # Ridge regression with L2 regularization
        m = len(y)
        self.weights = np.random.normal(0, 0.01, X.shape[1])
        self.bias = 0
        
        for i in range(1000):
            y_pred = self.predict(X)
            
            # Cost with regularization
            cost = np.mean((y - y_pred) ** 2) + self.alpha * np.sum(self.weights ** 2)
            
            # Gradients with regularization
            dw = (1/m) * np.dot(X.T, (y_pred - y)) + 2 * self.alpha * self.weights
            db = (1/m) * np.sum(y_pred - y)
            
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
```

> Th·ª±c h√†nh: thay ƒë·ªïi `alpha` ƒë·ªÉ th·∫•y t√°c ƒë·ªông c·ªßa regularization l√™n tr·ªçng s·ªë.

## üìù Notes Th·ª±c Thi D·ª± √Ån

- **Quy ∆∞·ªõc m√¥i tr∆∞·ªùng**: d√πng `lr_env` cho ri√™ng d·ª± √°n n√†y.
- **Ch·∫°y test tr∆∞·ªõc commit**: ƒë·∫£m b·∫£o `pytest` pass 100%.
- **Ki·ªÉm th·ª≠ th·ªß c√¥ng**: ch·∫°y notebook `02-simple-linear-regression` ƒë·ªÉ xem ƒë∆∞·ªùng h·ªìi quy v√† learning curve.
- **Quy t·∫Øc d·ªØ li·ªáu**: d·ªØ li·ªáu synthetic n·∫±m ·ªü `data/synthetic/`, d·ªØ li·ªáu th·∫≠t ·ªü `data/real_world/`.
- **Qu·∫£n l√Ω ph·ª• thu·ªôc**: c·∫≠p nh·∫≠t `requirements.txt` khi th√™m th∆∞ vi·ªán m·ªõi.
- **ƒê·ªãnh d·∫°ng code**: c√≥ th·ªÉ d√πng `black` v√† `flake8` (ƒë√£ khai b√°o trong requirements).

## üß™ H∆∞·ªõng D·∫´n Test Nhanh

```bash
python - << 'PY'
import numpy as np
from src.linear_regression import SimpleLinearRegression

X = np.linspace(-5,5,200).reshape(-1,1)
y = 2*X.squeeze() + 3 + np.random.normal(0,0.2,200)

model = SimpleLinearRegression(learning_rate=0.05, max_iterations=5000, tolerance=1e-10, random_state=42)
model.fit(X,y)
print(model.get_parameters())
print('R2=', model.score(X,y))
PY
```

## üéØ Expected Results

Sau khi ho√†n th√†nh d·ª± √°n n√†y, b·∫°n s·∫Ω c√≥:

1. **Deep Understanding**: Hi·ªÉu s√¢u v·ªÅ linear regression
2. **Implementation Skills**: T·ª± code ƒë∆∞·ª£c thu·∫≠t to√°n
3. **Mathematical Intuition**: Hi·ªÉu to√°n h·ªçc ƒë·∫±ng sau
4. **Optimization Knowledge**: Gradient descent variants
5. **Performance Comparison**: So s√°nh v·ªõi sklearn

## üîç Key Insights to Discover

### üìä Mathematical Insights
- **Cost function shape** ·∫£nh h∆∞·ªüng ƒë·∫øn convergence
- **Learning rate** quy·∫øt ƒë·ªãnh t·ªëc ƒë·ªô h·ªçc
- **Feature scaling** quan tr·ªçng cho gradient descent
- **Regularization** gi√∫p tr√°nh overfitting

### üéØ Implementation Insights
- **Vectorization** tƒÉng t·ªëc ƒë√°ng k·ªÉ
- **Batch size** ·∫£nh h∆∞·ªüng ƒë·∫øn stability
- **Initialization** quan tr·ªçng cho convergence
- **Early stopping** tr√°nh overfitting

## üìö T√†i Li·ªáu Tham Kh·∫£o

- [Andrew Ng's Machine Learning Course](https://www.coursera.org/learn/machine-learning)
- [Linear Regression Mathematics](https://en.wikipedia.org/wiki/Linear_regression)
- [Gradient Descent](https://en.wikipedia.org/wiki/Gradient_descent)
- [NumPy Documentation](https://numpy.org/doc/)

## üèÜ Next Steps

Sau khi ho√†n th√†nh Linear Regression from Scratch, b·∫°n c√≥ th·ªÉ:
- Implement other algorithms from scratch
- Chuy·ªÉn sang d·ª± √°n 4: Titanic Survival Prediction
- Th·ª≠ advanced optimization techniques
- Ph√°t tri·ªÉn mini ML library

## üé® Bonus: Interactive Demo

T·∫°o Streamlit app ƒë·ªÉ demo:
- Adjust learning rate v√† iterations
- Visualize gradient descent in real-time
- Compare different algorithms
- Upload custom datasets

---

**Happy Coding! üìà**

*H√£y b·∫Øt ƒë·∫ßu v·ªõi mathematical foundations v√† x√¢y d·ª±ng linear regression t·ª´ con s·ªë 0!*
